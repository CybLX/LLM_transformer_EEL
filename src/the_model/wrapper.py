#Autoregressive Wrapper
#To generate text one token at a time, we use an autoregressive wrapper. In an autoregressive model, the output from previous steps is fed as input to subsequent steps. This allows the model to predict the next token based on the tokens generated so far.
#
#The autoregressive wrapper takes a sequence of tokens as input, where the sequence length is one token longer than the maximum. This extra token is necessary because the model is trained to predict the next token in the sequence given the previous tokens.
#
#For example, consider the sentence: "The cat sat on the mat." If the input sequence is "The cat sat on the", the target output sequence (what the model is trained to predict) is "cat sat on the mat". The input sequence is shifted by one step to the left relative to the output sequence.
#
#The autoregressive wrapper also includes a method to calculate the probabilities for the next token in the sequence. These probabilities are generated by applying a softmax function to the logits (the raw, unnormalized scores) output by the model for each token in the vocabulary, based on the last token in the sequence.
#
#Additionally, a temperature parameter is used to adjust the sharpness of the probability distribution:
#
#Lower temperatures make the output more deterministic, increasing the likelihood of choosing the most probable token.
#Higher temperatures make the output more random, allowing for more variation in token selection.

from src.the_model.languague_model.language import LanguageModel
import torch

class AutoregressiveWrapper(torch.nn.Module):

    def __init__(self, gpt_model: LanguageModel):
        super().__init__()
        self.model = gpt_model
        self.max_sequence_length = self.model.max_sequence_length

    def forward(self, x: torch.Tensor, mask: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:

        inp, target = x[:, :-1], x[:, 1:]
        mask = mask[: , : -1]

        output = self.model(inp, mask)
        return output, target
    
    def next_token_probabilities(self, x: torch.Tensor, mask: torch.Tensor, temperature: float = 1.0) -> torch.Tensor:
        logits = self.model(x, mask)[:, -1]

        # Apply the temperature
        if temperature != 1.0:
            logits = logits / temperature

        # Apply the softmax
        probabilities = torch.softmax(logits, dim = -1)
        return probabilities
    
    def save_checkpoint(self, path: str) -> None:
        self.model.save_checkpoint(path)

    @staticmethod
    def load_checkpoint(path: str, cuda: torch.device) -> 'AutoregressiveWrapper':
        model = LanguageModel.load_checkpoint(path, cuda)
        return AutoregressiveWrapper(model).to(cuda)
